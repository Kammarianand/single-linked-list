{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b2434f",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff9a9e, #fad0c4); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">NLP (Natural Language Processing)</h1>\n",
    "\n",
    " <span style=\"background: linear-gradient(to right, #ff9a9e, #fad0c4); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Natural Language Processing, is a field that gives the machines the ability toread, understand, and derive meaning from human languages. It involves the interaction between computers and humans using the natural language to perform tasks like translation, sentiment analysis, speech recognition, etc</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c85199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272d301",
   "metadata": {},
   "source": [
    "<small> `Natural Language Toolkit`, is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources, along with a suite of text processing libraries for `classification`, `tokenization`, `stemming`, `tagging`, `parsing`, and `semantic reasoning`.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e9accc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8b9318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world, i use linux'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello world, i use linux\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba1b53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world,', 'i', 'use', 'linux']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d79c2c",
   "metadata": {},
   "source": [
    "* `word_tokenize()` splits a sentence into individual words\n",
    "* `sent_tokenize()` splits a text into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a864eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', ',', 'i', 'use', 'linux']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe40f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world, i use linux']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd346ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "i\n",
      "use\n",
      "linux\n"
     ]
    }
   ],
   "source": [
    "for words in word_tokenize(text):\n",
    "    if words != ',':\n",
    "        print(words)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955a253",
   "metadata": {},
   "source": [
    "<strong style='color:orange;font-size: 25px;'>Stemming and Lemmatizationn</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98ccbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0ad810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "stem = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca4cd4",
   "metadata": {},
   "source": [
    "### Lemmatization : \n",
    "<small>`lemmatization` involves reducing words to their dictionary form</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "766dd3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change\n",
      "changer\n",
      "change\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize('change'))\n",
    "print(lem.lemmatize('changer'))\n",
    "print(lem.lemmatize('changes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6e042",
   "metadata": {},
   "source": [
    "### Stemming :\n",
    "<small>`Stemming` is the process of reducing words to their root form</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861224c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "print(stem.stem(\"run\"))\n",
    "print(stem.stem('runner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ab016",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "<p>Stopwords are commonly used words in a language that are often filtered out in NLP <br>Examples : <br>\"is\", \"the\", \"and\", \"in\", \"it\", \"you\", \"to\", \"for\". </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20a3c047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d60e6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f125e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11708613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d502f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Using Linux is like having a secret superpower that makes Windows machines always crashing.\"\n",
    "text = word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e6ad75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "corpus = ''\n",
    "for word in text:\n",
    "    if word.lower() not in stopwords and len(word)>2:\n",
    "        corpus += word + \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5002ede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Linux is like having a secret superpower that makes Windows machines always crashing.\n",
      "Using Linux like secret superpower makes Windows machines always crashing \n"
     ]
    }
   ],
   "source": [
    "print(txt) # with stop words\n",
    "print(corpus) # without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98452663",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"where every user is a self-appointed expert and every system update feels like rolling the dice in a game of digital roulette. Installing software is like navigating a labyrinth of dependencies and package managers, with the occasional sacrifice to the Linux gods for good measure. Troubleshooting is an art form, involving hours of scouring obscure forums and deciphering cryptic error messages. And don't even get me started on hardware compatibility â€“ it's like playing Russian roulette with your peripherals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14317f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in word_tokenize(corpus):\n",
    "    if word.lower() not in stopwords and len(word)>2:\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0058e9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'art',\n",
       " 'compatibility',\n",
       " 'cryptic',\n",
       " 'deciphering',\n",
       " 'dependencies',\n",
       " 'dice',\n",
       " 'digital',\n",
       " 'error',\n",
       " 'even',\n",
       " 'every',\n",
       " 'expert',\n",
       " 'feels',\n",
       " 'form',\n",
       " 'forums',\n",
       " 'game',\n",
       " 'get',\n",
       " 'gods',\n",
       " 'good',\n",
       " 'hardware',\n",
       " 'hours',\n",
       " 'installing',\n",
       " 'involving',\n",
       " 'labyrinth',\n",
       " 'like',\n",
       " 'linux',\n",
       " 'managers',\n",
       " 'measure',\n",
       " 'messages',\n",
       " \"n't\",\n",
       " 'navigating',\n",
       " 'obscure',\n",
       " 'occasional',\n",
       " 'package',\n",
       " 'peripherals',\n",
       " 'playing',\n",
       " 'rolling',\n",
       " 'roulette',\n",
       " 'russian',\n",
       " 'sacrifice',\n",
       " 'scouring',\n",
       " 'self-appointed',\n",
       " 'software',\n",
       " 'started',\n",
       " 'system',\n",
       " 'troubleshooting',\n",
       " 'update',\n",
       " 'user'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "885e44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tk = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a13d98",
   "metadata": {},
   "source": [
    "<small>The `Tokenizer` class allows you to vectorize a text corpus, by turning each text into a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4d28b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Wolverine has Claws','Ironman has arcReactor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4211c",
   "metadata": {},
   "source": [
    "<small>`fit_on_texts()` is used to update the internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "35bbac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b365ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has': 1, 'wolverine': 2, 'claws': 3, 'ironman': 4, 'arcreactor': 5}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704f999",
   "metadata": {},
   "source": [
    "<small>`texts_to_sequences()` transforms each text in the given list of texts to a sequence of integers.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ce729669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 3], [4, 1, 5]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "441dcf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1, 'water': 2, 'cold': 3, 'black': 4, 'tea': 5, 'hot': 6}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer(num_words = 4)\n",
    "corps = ['water is cold','black tea is hot']\n",
    "tok.fit_on_texts(corps)\n",
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9adcd5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 3], [1]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.texts_to_sequences(corps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
